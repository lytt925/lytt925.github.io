{"posts":[{"title":"Blob Storage and Blob 筆記","text":"Blob Storage是一種用於儲存非結構化資料的雲端儲存服務，廣泛應用於各種場景，例如影像儲存、影片串流、資料備份等等。AWS S3、MinIO 等服務就是用來儲存 Blob 的 object storage，這篇文章將介紹Blob的概念、Blob Storage和傳統儲存方式的差異和好處。 什麼是 Blob在Object Storage中，Blob是儲存的基本單位，且彼此之間獨立。Blob沒有任何Schema或是格式的限制，他們可以是任何類型的資料，例如文字、影像、影片、音訊等等。雖然Blob通常被指涉成Binary Large Object，但似乎在Object Storage中，Blob只是指涉儲存的基本單位和概念，而不一定是二進制的資料。 在Blob Storage中，每個Blob會包含以下幾個部分： Data: 內容本身，如影片。 Metadata: 與內容相關的資訊，例如檔案大小、檔案類型等等。 Unique identifier: 用來識別Blob的唯一ID，可用來找到該blob。 Blob Storage我在Cloudflare的文件中找到了一篇介紹Blob Storage的文章，裡面有一個很好的比喻來說明Blob Storage和傳統的儲存方式的差異。裡面是這樣寫的： 想像一下，Alice 將她的衣服成套存放，非常方便穿戴，而 Bob 只是將他的衣服扔成一堆。Bob 的方法更像是 Blob 儲存體：任何一件衣服都可以放入他的堆中，並且不必以任何特定的方式整理衣服。Bob 的方法是有利的，因為他可以快速且幾乎無休止地往他的衣服堆上繼續放衣服：他只要繼續往上放就可以了，而不用像 Alice 那樣摺疊和整理它們。儘管 Bob 的服裝儲存方法讓他難以快速找到某一件特定的衣服，但許多組織需要類似的資料儲存方法。他們有大量資料，他們需要儲存大量資料，而無需將其組織到階層中或將其調整為給定格式。 相較於傳統階層式的儲存方式（像是資料夾），Blob Storage是以物件為儲存的基本單位，而每個物件都有一個ID可以讓使用者存取。通常被用來存圖片、影片、logs、備份、機器學習訓練資料。 這樣的儲存方式有幾個好處： Scalability更好，增加或減少伺服器就能做到水平擴展， 一個Object可以分散在不同的伺服器上，所以不會有單一伺服器的瓶頸。 因為每個Object都是獨立的，所以可以獨立地存取，彈性更高。 利用HTTP API可以直接存取Object，在開發上更方便。 更便宜有效率，用多少HTTP Request流量、儲存空間就算多少錢 缺點： 不能作為一般資料庫使用，因為太慢 不適合用於儲存需要頻繁更新的資料。 操作都是以Object為單位，所以沒辦法作部分更改(例如append a line要整個拿出來改再放回去)，有效能問題 特性 Blob Storage 傳統檔案儲存 可擴展性 非常好 較差 效能 較差 較好 成本 較低 較高 適用場景 影像儲存、影片串流、資料備份 頻繁操作的資料庫、精細的權限掌控 以上是我對Blob Storage的理解，如果有錯誤或是補充的地方，歡迎留言指教。 參考資料： What is blob storage Object Storage vs Block Storage","link":"/2024/02/18/Blob-Storage-and-Blob/"},{"title":"Vector DBs 筆記","text":"因為最近在搞RAG，所以會需要研究一下Vector Databases，這篇文章就來筆記一下vector databases的概念和用途。 Vector DatabasesEmbeddings所謂的embeddings 在自然語言處理 (NLP)中常被使用，因為電腦沒辦法處理一般的文字，所以需要先透過機器學習模型或是演算法將這些文字經過處理之後轉換成可以代表這些文字的向量。如果兩串文字很接近，那麼他們的向量也會很接近，接近程度可以用Euclidean distance, cosine similarity等指標來衡量。 當我們有了文字的embedding，電腦可以利用這些向量來表徵文字之後，就可以進行運算和訓練機器學習模型。 當然，也不是只有文字可以轉換成embedding，圖片、音訊等也可以轉換成embedding。 Why Vector Databases不同於一般資料庫 (MySQL, MongoDB, etc.)會儲存純量值，Vector Databases則是專門用來儲存多維度的資料點，即向量 (vector)。這些vector dbs會針對這些向量做一些特別的indexing和query機制，或是實作一些基於向量的計算 (e.g. similarity search)。 What can Vector Databases do?一個Vector Database可以讓我們找到與query的文字vector相近的資料。一開始會先將一個query vector傳入vector db，然後vector db就會回傳一組資料庫中與query vector最接近的資料。 藉此就可以用資料庫來做到語意的搜尋、分類、content-based的推薦系統等功能。所以RAG也會用到，因為RAG就是先利用prompt從資料庫中找到相近的資料然後一起餵給LLM。 實際的流程大概會像這樣： 先將已有的資料透過一個機器學習模型轉為一堆向量 將這些向量放到vector dbs，會用一些特別的indexing機制 將要查詢的文字透過與第一點一樣的模型先轉為向量。 利用這組向量來找資料庫中最接近的向量，這個步驟仰賴資料庫本身的query language或是API，以及用來計算相似度的指標 透過這個向量來找到原始文字 常見的vector DBs 圖片來源: https://www.datacamp.com/blog/the-top-5-vector-databases 參考資料： What are Vector Databases and How Langchain uses Vector DBs Vector databases","link":"/2024/02/21/vectorDBs-note/"},{"title":"Retrieval Augmented Generation (RAG) 介紹","text":"LLM具有強大的自然語言處理能力，許多應用也開始加入LLM來提升與人的互動能力。然而LLM也有其侷限性。他可能會產生幻覺且不會有來源佐證，且資訊不一定是up to date的，因為他的訓練資料以經是固定的。這樣的侷限性會導致沒辦法好好利用他很厲害的語言能力，所以有一些研究者提出了一些方法來解決這個問題，其中一個方法就是使用Retriever Augmented Generation (RAG)。 如何讓如何讓LLM更精準跟即時？LLM的問題有幾個： 沒有source，可能會有hallucination的問題 不即時、資訊不up to date 那麼存在的Specialized方式有： Instruction Fine-tuning Reinforcement Learning from human feedback (RLFH) Retriever Augmented Generation (RAG) 相較於fine-tune或重新訓練，RAG的優點可以快速解決上面的兩點問題，且不需要花費太多時間和運算資源。 RAG是什麼？RAG透過結合檢索（Retrieval）和生成（Generation）的方法來擴展LLM的能力。其工作流程可以分為以下幾個步驟： Retrieval 階段：當收到一個查詢（query/prompt）時，RAG首先從一個 vector database 中取出相關的資訊（詳細可以看該連結）。這些資訊就是用來幫助LLM生成更精準的回答。 Augmented 階段：接著，RAG將找到的內容與prompt一起整合成一個新的輸入（prompt）。這個prompt會包含： retrieved content The instruction to pay attention to retrieved content and to give evidence The user’s original question Generation 生成階段：最後，這個整合後的輸入會餵給LLM。模型利用retrieve到的資訊和其自身強大的自然語言對話能力，就可以生成一個既考慮了User問題又融合了相關證據的回答。","link":"/2024/02/22/RAG-intro/"},{"title":"在LangChain使用LLM API自訂客製化的LLM","text":"最近必須使用一個LLM的API來做一個RAG應用，但是LangChain沒辦法直接接外部的LLM API，必須自己實作一個客製化的LLM Class來配合LangChain其他的功能。這篇文章將會介紹如何在LangChain使用外部的LLM API自訂LangChain的LLM Class。 1. 安裝LangChain123pip install langchain# orconda install langchain -c conda-forge 2. 建立自訂的LLM Class需要透過繼承langchain.llms.base.LLM來建立自訂的LLM Class，並實作以下的方法： _call：用來呼叫LLM API的方法，並回傳結果。 _stream (optional)：用來呼叫LLM API的方法，會回傳一個Generator，來做到Streaming的效果。 _astream (optional)：用來呼叫LLM API的方法，並回傳一個AsyncGenerator，來做到Async Streaming的效果。 如果沒有2. _stream 和 3. _astream 的話，LangChain會自動使用_call的方法來呼叫LLM API。 詳細請見： https://python.langchain.com/docs/modules/model_io/llms/custom_llm https://python.langchain.com/docs/modules/model_io/chat/custom_chat_model 裡面可能可以更好的地方 因為api是透過Bearer Token來驗證，所以每次呼叫我都先取得JWT Token，然後再透過Token來呼叫LLM API。 在_stream 和 _astream的方法裡面，LLM的response是透過requests和httpx來取得Server Sent Event的response，然後再用這些response做出generator。我對python的async不太熟，不知道有沒有更好的寫法。 基本上，這個LLM Class就是一個對外部LLM API的包裝，讓LangChain可以使用外部的LLM API。之後就可以直接instanitate這個LLM Class，然後就可以使用LangChain的其他功能了。像是RAG Chain等等。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190import jsonimport osimport httpximport requestsimport asynciofrom typing import Optional, List, Any, Mapping, Iterator, Dict, AsyncIteratorfrom langchain.callbacks.manager import CallbackManagerForLLMRun from langchain.schema.output import GenerationChunk from langchain.llms.base import LLMfrom langchain.callbacks.base import BaseCallbackHandlerfrom langchain.schema import LLMResultclass TaideLLM(LLM): llm_host = 'https://td.nchc.org.tw' llm_url = f'{llm_host}/api/v1' # or whatever your REST path is... llm_type = &quot;TAIDE/b.11.0.0&quot; username = os.getenv('TAIDE_USERNAME') password = os.getenv('TAIDE_PASSWORD') @property def _llm_type(self) -&gt; str: return self.llm_type @property def _identifying_params(self) -&gt; Mapping[str, Any]: &quot;&quot;&quot;Get the identifying parameters.&quot;&quot;&quot; return {&quot;llmUrl&quot;: self.llm_url} def _get_token(self) -&gt; str: # get token r = requests.post(self.llm_url+&quot;/token&quot; , data={&quot;username&quot;: self.username, &quot;password&quot;: self.password }) token = r.json()[&quot;access_token&quot;] return token def _stream_response_to_generation_chunk( self, stream_response: Dict[str, Any], ) -&gt; GenerationChunk: &quot;&quot;&quot;Convert a stream response to a generation chunk.&quot;&quot;&quot; if not stream_response[&quot;choices&quot;]: return GenerationChunk(text=&quot;&quot;) return GenerationChunk( text=stream_response[&quot;choices&quot;][0][&quot;text&quot;], generation_info=dict( finish_reason=stream_response[&quot;choices&quot;][0].get(&quot;finish_reason&quot;, None), logprobs=stream_response[&quot;choices&quot;][0].get(&quot;logprobs&quot;, None), ), ) def _call( self, prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any, ) -&gt; str: if stop is not None: raise ValueError(&quot;stop kwargs are not permitted.&quot;) # chat question = prompt prompt_1 = f&quot;[INST] {question} [/INST]&quot; data = { &quot;model&quot;: self.llm_type, &quot;prompt&quot;: prompt_1, &quot;temperature&quot;: 0.2, &quot;top_p&quot;: 0.9, &quot;presence_penalty&quot;: 1, &quot;frequency_penalty&quot;: 1, &quot;max_tokens&quot;: 2000 } headers = { &quot;Authorization&quot;: &quot;Bearer &quot;+ self._get_token() } r = requests.post(self.llm_url+'/completions', headers=headers, json=data) r.raise_for_status() return r.json()[&quot;choices&quot;][0][&quot;text&quot;] def _stream( self, prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any, ) -&gt; Iterator[GenerationChunk]: question = prompt prompt_1 = f&quot;[INST] {question} [/INST]&quot; data = { &quot;model&quot;: self.llm_type, &quot;prompt&quot;: prompt_1, &quot;temperature&quot;: 0.2, &quot;top_p&quot;: 0.9, &quot;presence_penalty&quot;: 1, &quot;frequency_penalty&quot;: 1, &quot;max_tokens&quot;: 2000, &quot;stream&quot;: True } headers = { &quot;Authorization&quot;: &quot;Bearer &quot;+ self._get_token() } with requests.post(self.llm_url+&quot;/completions&quot;, stream=True, json=data, headers=headers) as r: r.raise_for_status() for raw_chunk in r.iter_lines(): if raw_chunk[0:6] == &quot;data: &quot;: json_str = raw_chunk[6:] # Strip 'data: ' to get the JSON part # Step 3: Parse the JSON string into a Python dictionary try: data_dict = json.loads(json_str) # Step 4: Access elements in the dictionary chunk = self._stream_response_to_generation_chunk(data_dict) print(chunk.text) yield chunk if run_manager: run_manager.on_llm_new_token( chunk.text, chunk=chunk, verbose=self.verbose, logprobs=( chunk.generation_info[&quot;logprobs&quot;] if chunk.generation_info else None ), ) except json.JSONDecodeError: continue async def _astream( self, prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any, ) -&gt; AsyncIterator[GenerationChunk]: question = prompt prompt_1 = f&quot;[INST] {question} [/INST]&quot; data = { &quot;model&quot;: self.llm_type, &quot;prompt&quot;: prompt_1, &quot;temperature&quot;: 0.2, &quot;top_p&quot;: 0.9, &quot;presence_penalty&quot;: 1, &quot;frequency_penalty&quot;: 1, &quot;max_tokens&quot;: 2000, &quot;stream&quot;: True } headers = { &quot;Authorization&quot;: &quot;Bearer &quot;+ self._get_token() } async with httpx.AsyncClient(timeout=600) as client: async with client.stream(&quot;POST&quot;, f&quot;{self.llm_url}/completions&quot;, json=data, headers=headers) as response: response.raise_for_status() async for raw_chunk in response.aiter_lines(): if raw_chunk.startswith('data:'): # Check if the chunk starts with 'data:' json_str = raw_chunk[6:] else: continue # Step 3: Parse the JSON string into a Python dictionary try: data_dict = json.loads(json_str) # Step 4: Access elements in the dictionary chunk = self._stream_response_to_generation_chunk(data_dict) yield chunk if run_manager: await run_manager.on_llm_new_token( chunk.text, chunk=chunk, verbose=self.verbose, logprobs=( chunk.generation_info[&quot;logprobs&quot;] if chunk.generation_info else None ), ) except json.JSONDecodeError: continue","link":"/2024/02/25/Custom-LLM-in-LangChain/"},{"title":"用Github Actions 和 GIS 來自動部署Hexo和建立網站索引","text":"在用Hexo建立部落格之後，第一件重要的事情就是要減少除了寫文章以外的事情例如部屬等等，也會希望Google可以將自己的部落格建立索引 (index) 。這篇文章會介紹如何使用Hexo + Google Indexing Script + Github Actions來達成這個目標。 建立 Github Actions Workflow 來自動部署Githuab Actions是一個可以自動化執行一些任務的工具，例如測試、部署等等。開發者可以自由設定任務的觸發條件，例如push到master branch後、每天定時執行等等。在這個例子裡面，我們會使用Github Actions來自動化執行部署和建立索引的任務。首先我們先專注在hexo部署的部分。 在我的例子裡面，我使用了Github Pages來部署我的部落格，也就是存放hexo產生後的靜態檔案的repo，是一個會叫做username.github.io的repo。我的Hexo Source Code則是存在另外一個private的repo，Github Actions就是要在這個repo裡面建立和執行的。在我每次 hexo new post 後並把文章寫好後，我會push到這個private repo，然後Github Actions就會自動執行部署的任務，產生靜態檔案，並且deploy到Github Pages的repo裡面。 要進行下面的步驟之前，請先確定你的hexo部署是正常的，也就是當你直接在local執行 hexo deploy 後，你的部落格是可以正常在Github Pages上面顯示的。 1. 取得 Deploy Key首先在你的local端產生一個新的ssh key，並且將這個key加入到你的Github Pages repo的deploy key裡面。輸入以下指令： 1ssh-keygen -t ed25519 -C &quot;&lt;email&gt;&quot; -f ga-deploy 你可以改變這個key的名稱，這個key是用來讓Github Actions可以push到你的Github Pages repo的。 你應該會獲得兩個檔案，一個是id_ed25ga-deploy519，一個是ga-deploy.pub。ga-deploy是private key，ga-deploy.pub是public key。接著你可以將ga-deploy.pub的內容加入到你的Github Pages repo的deploy key裡面。這樣Github Actions就可以使用這個key來push到你的Github Pages repo。 2. 將Public Key加入到Github Pages repo的Deploy Key裡面輸入cat /path/to/ga-deploy.pub，然後複製這個public key的內容。接著到你的Github Pages repo的deploy key頁面，新增一個deploy key，並且將這個public key的內容貼上去。 3. 將Private Key加入到Source Code Repo 的 Github Actions 的 Secrets裡面進入 Hexo Source Code的repo，然後點選Settings -&gt; Secrets and variables -&gt; Actions，然後新增一個名為DEPLOY_KEY的secret，並且將private key ga-deploy 的內容貼上去。 1cat /path/to/ga-deploy 4. 在 Source Code Repo 建立一個新的 Github Actions workflow 檔案首先在我的Github Repository裡面建立一個新的workflow檔案，檔案名稱為.github/workflows/google-indexing.yml。裡面要自動化的步驟如下： 取得source code (actions/checkout) 安裝node.js (actions/setup-node) 設定環境 (Configuration environment) 這裡會將 DEPLOY_KEY 的secret取出來，並且設定到 HEXO_DEPLOY_PRI 的環境變數裡面。 同時設定git的使用者名稱和email等等。 還原檔案的修改時間 (Restore file modification time) 這是為了讓hexo deploy後的檔案的修改時間是正確的。否則checkout後的檔案的修改時間會是現在的時間。部落格的文章會根據修改時間來顯示和排序，所以很重要。 安裝hexo的相依套件 (Install dependencies) 部署hexo (Deploy hexo) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849name: HEXO CIon: [push]jobs: build: runs-on: ubuntu-latest strategy: matrix: node-version: [21.x] steps: - uses: actions/checkout@v4 with: fetch-depth: 0 - name: Use Node.js ${{ matrix.node-version }} uses: actions/setup-node@v4 with: node-version: ${{ matrix.node-version }} - name: Configuration environment env: HEXO_DEPLOY_PRI: ${{secrets.DEPLOY_KEY}} run: | mkdir -p ~/.ssh/ echo &quot;$HEXO_DEPLOY_PRI&quot; &gt; ~/.ssh/id_rsa chmod 600 ~/.ssh/id_rsa ssh-keyscan github.com &gt;&gt; ~/.ssh/known_hosts git config --global user.name &quot;lytt925 (via GitHub Actions)&quot; git config --global user.email &quot;ytli.tw@gmail.com&quot; - name: Restore file modification time run: | find source -name '*.md' | while read file; do new_time=$(git log -1 --format=&quot;%ct&quot; &quot;$file&quot;) # Remove the extra @ touch -d &quot;@$new_time&quot; &quot;$file&quot; # Correct usage of @ for touch command echo &quot;Updated $file to $(date -d @$new_time)&quot; # Correct usage of @ for date command done - name: Install dependencies run: | npm i -g hexo-cli npm i - name: Deploy hexo run: | hexo generate hexo deploy 5. Push到Github Source Code Repo到這裡，將這個workflow檔案push到你的Github Source Code Repo裡面，然後Github Actions就會自動執行這個workflow。你可以在Actions頁面看到這個workflow的執行狀況。你可以隨便改一個文章來測試這個workflow是否正常執行。 加入 Google Indexing Script我們開始寫部落格之後，也會希望Google可以將自己的部落格建立索引 (index) ，這樣別人搜尋的時候就可以找到你的部落格。這裡我們會使用Google Indexing Script來達成這個目標。 1. 先去Google Search Console 驗證你的部落格你可以在右邊打上你github pages的網址，然後點選驗證，然後選擇一個方法來驗證。 驗證成功後，你就可以在Google Search Console裡面看到你的部落格的資訊。 2. Google Indexing Script在Google Search Console有個網址審查的功能，你可以將你的部落格url或是某篇文章的url進行審查，那麼過不久就可以在google search搜尋到，你可以在搜尋引擎打上site:your-blog-url來看看你的部落格有沒有被建立索引。但因為文章很多的話，這樣一篇一篇點會很麻煩，所以我們使用Google Indexing Script 來自動化這個過程。他是利用Google Search Console的API來達成這個目標（應該），你可以看他的github頁面來了解更多。 使用這個script有幾個先決條件： 你的網站需要在google search console驗證過 你的網站需要有一個Sitemap 你需要在Google Cloud Platform建立一個新的Project並且取得一個Service Account的 API Key 你需要在google search console裡面加入這個API Key，並且給予他網站擁有者的權限 2.1. 取得網站的Sitemap Site map是一個xml檔案，裡面包含了你部落格的所有文章的url和相關資訊，這樣google就可以從這個檔案裡面找到你的文章。 可以參考這篇 懶人包： npm install hexo-generator-sitemap 在_config.yml裡面加入 12sitemap:path: sitemap.xml 你可以在your-blog-url/sitemap.xml看到你的sitemap 在google search console裡面加入sitemap 2.2. 在Google Cloud Platform建立一個新的Project在https://console.cloud.google.com/ 建立一個新的Project，然後在這個Project裡面建立一個Service Account，並且取得一個API Key。 直接按左上角的Project欄位來新增一個新的Project 2.3. 建立服務帳戶 service account 並且取得API Key 點選左邊的IAM &amp; Admin -&gt; 服務帳戶 Service Accounts -&gt; 建立服務帳戶 Create Service Account 輸入名稱後直接點選Save就好 點選剛剛建立的服務帳戶，然後點選金鑰 Keys -&gt; 新增金鑰 Create Key -&gt; JSON 這樣你就會下載到一個json檔案，這個檔案就是你的API Key 把這個json檔案放到你的Github Source Code Repo的Secrets裡面，並且取名叫做service_account.json。 2.4. 在 Google Search Console 加入 API Key打開你的service_account.json，把裡面的client_email複製下來，然後到Google Search Console裡面，點選左邊的設定 -&gt; 使用者與權限 -&gt; 新增使用者，然後把剛剛複製下來的client_email貼上去，並且選擇擁有者的權限。 2.5. 在 Github Action 加入 Google Indexing Script按照一樣的方式，將service_account.json的內容加入到你的Github Source Code Repo的Secrets裡面，並且取名叫做SERVICE_ACCOUNT_JSON。然後新增一個流程到剛剛的workflow裡面。 gis 後面要加入你的部落格網址，再指定--path參數為你的service_account.json的路徑。 1234567- name: Google Indexing Script env: SERVICE_ACCOUNT_JSON: ${{secrets.SERVICE_ACCOUNT_JSON}} run: | npm i -g google-indexing-script echo &quot;$SERVICE_ACCOUNT_JSON&quot; &gt; ./service_account.json gis blog.ytli.tw --path ./service_account.json 完整的workflow檔案如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859name: HEXO CIon: [push]jobs: build: runs-on: ubuntu-latest strategy: matrix: node-version: [21.x] steps: - uses: actions/checkout@v4 with: fetch-depth: 0 - name: Use Node.js ${{ matrix.node-version }} uses: actions/setup-node@v4 with: node-version: ${{ matrix.node-version }} - name: Configuration environment env: HEXO_DEPLOY_PRI: ${{secrets.DEPLOY_KEY}} run: | mkdir -p ~/.ssh/ echo &quot;$HEXO_DEPLOY_PRI&quot; &gt; ~/.ssh/id_rsa chmod 600 ~/.ssh/id_rsa ssh-keyscan github.com &gt;&gt; ~/.ssh/known_hosts git config --global user.name &quot;lytt925 (via GitHub Actions)&quot; git config --global user.email &quot;ytli.tw@gmail.com&quot; - name: Restore file modification time run: | find source -name '*.md' | while read file; do new_time=$(git log -1 --format=&quot;%ct&quot; &quot;$file&quot;) # Remove the extra @ touch -d &quot;@$new_time&quot; &quot;$file&quot; # Correct usage of @ for touch command echo &quot;Updated $file to $(date -d @$new_time)&quot; # Correct usage of @ for date command done - name: Install dependencies run: | npm i -g hexo-cli npm i - name: Deploy hexo run: | hexo generate touch public/CNAME echo &quot;blog.ytli.tw&quot; &gt; public/CNAME hexo deploy - name: Google Indexing Script env: SERVICE_ACCOUNT_JSON: ${{secrets.SERVICE_ACCOUNT_JSON}} run: | npm i -g google-indexing-script echo &quot;$SERVICE_ACCOUNT_JSON&quot; &gt; ./service_account.json gis blog.ytli.tw --path ./service_account.json 結果到這裡，你的部落格就會在每次部署後自動建立索引了。你可以在Actions頁面看到這個workflow的執行狀況。","link":"/2024/03/12/hexo-google-indexing-script-github-actions/"},{"title":"TypeScript 專案設定與開發流程","text":"最近才開始用TypeScript開發，這篇文章記錄一下我做的設定，想要達成像是一般JavaScript的開發體驗，將所有TypeScript編譯的步驟自動化。 TypeScript 專案設定以下會從零開始建立一個TypeScript專案，並且設定好所有的開發流程。 1. 初始化專案使用npm init來初始化一個新的專案。 12345mkdir my-ts-projectcd my-ts-projectnpm init -ymkdir srctouch src/index.ts 2. 安裝 TypeScript 和 初始化 TypeScript 設定檔安裝TypeScript，如果你不想要globally安裝，可以使用--save-dev的方式安裝。不過之後使用TypeScript的指令就要改成npx tsc。 123npm install -g typescript# 或者# npm install --save-dev typescript 接著初始化TypeScript的設定檔。我是globally安裝的，所以可以直接使用tsc指令來初始化。 1tsc --init 此時會產生一個tsconfig.json的檔案，這個檔案是用來設定TypeScript的編譯設定。他有很多預設的設定，我大多也是沿用，只有一些微調，以下是所有沒有comment起來的部分。詳細可以看 tsconfig reference。 123456789101112131415161718{ &quot;compilerOptions&quot;: { &quot;target&quot;: &quot;es2016&quot;, &quot;module&quot;: &quot;NodeNext&quot;, &quot;rootDir&quot;: &quot;src&quot;, &quot;moduleResolution&quot;: &quot;NodeNext&quot;, &quot;sourceMap&quot;: true, &quot;outDir&quot;: &quot;dist&quot;, &quot;esModuleInterop&quot;: true, &quot;forceConsistentCasingInFileNames&quot;: true, &quot;strict&quot;: true, &quot;noImplicitAny&quot;: true, &quot;skipLibCheck&quot;: true }, &quot;include&quot;: [ &quot;src/**/*&quot; ]} 3. 建立一個簡單的Express Server安裝需要的套件，這裡我們需要安裝@types/node，因為我們要使用Node.js的一些API。另外我們再安裝express。 123npm install --save-dev @types/nodenpm install expressnpm install --save-dev @types/express 建立一個src/index.ts檔案，並且貼上以下程式碼。 12345678910111213import express from 'express';import { Request, Response } from 'express';const app = express();const port = 3000;app.get('/', (req: Request, res: Response) =&gt; { res.send('Hello World!');});app.listen(port, () =&gt; { console.log(`Server started at http://localhost:${port}`);}); 這時候你的檔案結構應該是這樣的： 1234567.├── node_modules├── package-lock.json├── package.json├── src│ └── index.ts└── tsconfig.json 試著執行tsc，你會發現dist資料夾被建立，裡面有index.js和index.js.map。 1tsc 再執行node dist/index.js，你應該會看到Server started at http://localhost:3000，且在瀏覽器打開http://localhost:3000會看到Hello World!。 到這裡已經建立好一個TypeScript的專案，並且可以正常運行。 設定開發流程開發流程我們會希望有以下功能： 當我修改src/index.ts時，自動重新編譯。 當我修改src/index.ts時，自動重新啟動server。 可以根據環境變數來決定要使用哪一個設定檔。 1. 安裝套件我們需要安裝nodemon來達成自動重新編譯和重新啟動server的功能。另外再安裝concurrently，這樣我們可以同時執行多個指令。 1npm install --save-dev nodemon concurrently 2. 加入.env檔案這邊我們會加入.env.development和.env.production，分別是開發和正式環境的設定檔。這樣我們可以根據環境變數來決定要使用哪一個設定檔。你可以使用dotenv套件再根據NODE_ENV來讀取這些設定檔，不過這裡我們打算直接使用--env-file的方式來讀取。 123456touch .env.developmenttouch .env.productionecho &quot;NODE_ENV=development&quot; &gt; .env.developmentecho &quot;PORT=3000&quot; &gt;&gt; .env.developmentecho &quot;NODE_ENV=production&quot; &gt; .env.productionecho &quot;PORT=8080&quot; &gt;&gt; .env.production 我們也把src/index.ts的port部分改成以下程式碼，這樣我們可以根據環境變數來決定要使用哪一個設定檔。 1const port = process.env.PORT || 3000; 2. 修改package.json 加入prestart指令，在執行start之前會先刪除dist資料夾，再重新編譯。 加入start指令，這是給正式環境使用的，會讀取.env.production的設定檔。且因為有prestart指令，所以會先重新編譯。 加入watch:ts指令，這是給開發環境使用的，會監聽src資料夾的變化，並且重新編譯。 加入dev指令，這是給開發環境使用的，會同時執行watch:ts和nodemon，中間利用&amp;來做到同時執行，缺點是其中一邊的output會蓋過另外一邊的output，這個可以透過concurrently解決，可以參考concurrently。 加入 `”type”: “module” 來使用 ESM 的方式來import module。 另外，nodemon也可以設定額外參數，例如--exec來指定要執行的指令，這樣我們就可以直接執行node，並且讀取.env.development的設定檔。也可以利用--watch dist來指定只要監聽dist的變化就好。除了command line參數，nodemon也可以透過nodemon.json來設定，這樣指令看起來會短一點，不過我喜歡把–exec的部分寫在package.json 12345678910111213141516171819202122232425{ &quot;name&quot;: &quot;my-ts-project&quot;, &quot;version&quot;: &quot;1.0.0&quot;, &quot;description&quot;: &quot;&quot;, &quot;main&quot;: &quot;index.js&quot;, &quot;type&quot;: &quot;module&quot;, &quot;scripts&quot;: { &quot;test&quot;: &quot;echo \\&quot;Error: no test specified\\&quot; &amp;&amp; exit 1&quot;, &quot;build&quot;: &quot;tsc&quot;, &quot;prestart&quot;: &quot;rm -rf dist &amp;&amp; npm run build&quot;, &quot;start&quot;: &quot;node --env-file=.env.production --enable-source-maps dist/index.js&quot;, &quot;watch:ts&quot;: &quot;tsc -w&quot;, &quot;watch:node&quot;: &quot;nodemon --exec node --env-file=.env.development --enable-source-maps dist/index.js&quot;, &quot;dev&quot;: &quot;tsc &amp;&amp; concurrently -k \\&quot;npm run watch:ts\\&quot; \\&quot;npm run watch:node\\&quot;&quot; }, &quot;keywords&quot;: [], &quot;author&quot;: &quot;&quot;, &quot;license&quot;: &quot;ISC&quot;, &quot;dependencies&quot;: { &quot;express&quot;: &quot;^4.19.2&quot; }, &quot;devDependencies&quot;: { &quot;nodemon&quot;: &quot;^3.1.0&quot; }} 執行專案到這裡我們的開發流程已經設定好了，可以執行以下指令來開始開發。 1npm run dev 執行結果： 如果到時候要用production環境執行或是寫Dockerfile，可以執行以下指令。 1npm run start 執行結果：可以看到port已經改成8080了。","link":"/2024/04/19/setup-typescript-express-project/"},{"title":"如何在 TypeScript 專案中加入 eslint 和 prettier","text":"在建立完一個基本的 TypeScript Project 之後，我們希望可以加入 eslint 來做程式碼品質的管控，並且加入 prettier 來統一程式碼的格式。這篇文章將會介紹如何在 TypeScript Project 中加入 eslint 和 prettier。 安裝 eslint 和 設定檔現在 eslint 有提供一個很簡單的方式來安裝，只需要執行以下指令即可： 1npm init @eslint/config@latest 接著會詢問一些問題，像是這樣： 12345678910✔ How would you like to use ESLint? · problems✔ What type of modules does your project use? · esm✔ Which framework does your project use? · none✔ Does your project use TypeScript? · typescript✔ Where does your code run? · nodeThe config that you've selected requires the following dependencies:eslint@9.x, globals, @eslint/js, typescript-eslint✔ Would you like to install them now? · No / Yes✔ Which package manager do you want to use? · npm 根據他的指示來回答之後，會安裝了 eslint、globals、@eslint/js、typescript-eslint 這些套件，最後會產生一個 eslint.config.js 或是 ``eslint.config.mjs` 檔案，裡面會有 eslint 的設定檔。按照上面的回答的話，config 檔案會長這樣： 12345678910import globals from &quot;globals&quot;;import pluginJs from &quot;@eslint/js&quot;;import tseslint from &quot;typescript-eslint&quot;;export default [ { files: [&quot;**/*.{js,mjs,cjs,ts}&quot;] }, { languageOptions: { globals: globals.node } }, pluginJs.configs.recommended, ...tseslint.configs.recommended,]; eslint 在 v9.0.0 以後都推薦採用 eslint.config.js 這種 flat config 來設定，所以不再需要使用 .eslintrc 這種檔案來設定 eslint。可以參考這篇文章 來了解更多。 在上面的設定檔中，globals 是用來設定全域變數的，pluginJs 是用來設定 JavaScript 的規則，tseslint 是用來設定 TypeScript 的規則。裡面的 recommended 是指 eslint 推薦的規則。從官方文件 中可以看到這些規則包含的內容。其中有打勾的是 eslint 推薦的規則。而 typescript-eslint 也有提供一些規則 可以根據自己的需求引入。例如將 tseslint.configs.recommended 改成 tseslint.configs.strict 就會引入更多的規則。而這兩個規則都沒有做 type-checking，如果要加入的話可以改用 tseslint.configs.recommendedTypeChecked 或 tseslint.configs.stylisticTypeChecked。 如果要在 command line 中執行 eslint 的話，可以使用以下指令： 1npx eslint . 除了安裝 eslint 套件本身，也可以安裝 eslint 的 vscode 擴充套件，這樣在 vscode 中就可以直接看到 eslint 的警告。 安裝 prettier 和 設定檔接著我們要安裝 prettier，只需要執行以下指令即可，如果安裝失敗的話可以加上 --legacy-peer-deps 來避免 peer dependencies 的問題。這裡的 eslint-config-prettier 是用來關閉 eslint 中與 prettier 衝突的規則，因為 Linters 通常也會有一些格式化的規則，這樣會造成 prettier 和 eslint 之間的衝突，所以需要這個套件來關閉這些規則。Prettier 和 Linter 之間的差異可以參考這篇文章。大意是在說 formatting 的部分交給 prettier，而 Linter 則是用來檢查程式碼的品質。 12npm install --save-dev --save-exact prettier --legacy-peer-depsnpm install --save-dev eslint-config-prettier --legacy-peer-deps 接著將 prettier 的設定檔加入到 eslint.config.js 中： 12345678910111213import globals from &quot;globals&quot;;import pluginJs from &quot;@eslint/js&quot;;import tseslint from &quot;typescript-eslint&quot;;import eslintConfigPrettier from &quot;eslint-config-prettier&quot;;export default [ {files: [&quot;**/*.{js,mjs,cjs,ts}&quot;]}, {languageOptions: { globals: globals.node }}, pluginJs.configs.recommended, ...tseslint.configs.recommended, eslintConfigPrettier]; 這裡的 eslintConfigPrettier 是用來關閉 eslint 中與 prettier 衝突的規則。 接著我們要加入 prettier 的設定檔，只需要在專案根目錄下新增一個 .prettierrc 檔案，裡面可以設定 prettier 的規則。因為 prettier 本身是一個 opinionated 的工具，他們的設計理念就是不要花太多時間在討論風格，所以只要使用最基本的設定就可以了，例如： 1234{ &quot;tabWidth&quot;: 2, &quot;useTabs&quot;: false,} 這篇關於 prettier 的官方文件 有提供所有的設定選項。而在這之前也可以看一下他們的選項哲學。 如果要在 command line 中執行 prettier 的話直接做 formatting 的話，可以使用以下指令： 1npx prettier --write . 這樣就完成了 prettier 的設定。也可以安裝 prettier 的 vscode 擴充套件，這樣可以利用 vscode 的快捷鍵來做 formatting，而擴充套件的設定檔會是目前工作目錄下的，而 prettier 本身也是用 locally 安裝的 prettier，如果沒有安裝那就會用擴充套件附帶的 prettier，通常要避免這種情況，且如果要進行 Continuous Integration 的話安裝 prettier 在專案裡面是必須的。","link":"/2024/06/29/eslint-and-prettier-in-typescript-project/"}],"tags":[{"name":"Blob","slug":"Blob","link":"/tags/Blob/"},{"name":"Backend","slug":"Backend","link":"/tags/Backend/"},{"name":"Web Development","slug":"Web-Development","link":"/tags/Web-Development/"},{"name":"S3","slug":"S3","link":"/tags/S3/"},{"name":"MinIO","slug":"MinIO","link":"/tags/MinIO/"},{"name":"LLM","slug":"LLM","link":"/tags/LLM/"},{"name":"RAG","slug":"RAG","link":"/tags/RAG/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"LangChain","slug":"LangChain","link":"/tags/LangChain/"},{"name":"TypeScript","slug":"TypeScript","link":"/tags/TypeScript/"},{"name":"ESLint","slug":"ESLint","link":"/tags/ESLint/"},{"name":"Prettier","slug":"Prettier","link":"/tags/Prettier/"}],"categories":[{"name":"Backend","slug":"Backend","link":"/categories/Backend/"},{"name":"LLM","slug":"LLM","link":"/categories/LLM/"},{"name":"Web Development","slug":"Web-Development","link":"/categories/Web-Development/"}],"pages":[{"title":"","text":"","link":"/about/index.html"}]}